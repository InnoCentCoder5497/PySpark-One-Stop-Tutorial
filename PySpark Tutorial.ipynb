{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4c86f1-4779-48ca-be7b-5c2d8ad2ebe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PySpark\n",
    "PySpark is a Python API for Apache Spark. Apache Spark is an analytical processing engine designed for large-scale, high-performance distributed data processing and machine learning applications. \n",
    "\n",
    "PySpark is widely used in the Data Science and Machine Learning communities, as there are many popular data science libraries written in Python, such as NumPy and TensorFlow.\n",
    "It is also used because of its efficiency in processing large datasets.\n",
    "Many organisations, including Walmart, Trivago, Sanofi, Runtastic, and others, have used PySpark. \n",
    "\n",
    "## Architecture\n",
    "Workflow of Spark Architecture,\n",
    "\n",
    "**STEP 1**\n",
    "The client submits spark user application code. When an application code is submitted, the driver implicitly converts user code that contains transformations and actions into a logically directed acyclic graph called DAG. At this stage, it also performs optimizations such as pipelining transformations.\n",
    "\n",
    "**STEP 2**\n",
    "After that, it converts the logical graph called DAG into physical execution plan with many stages. After converting into a physical execution plan, it creates physical execution units called tasks under each stage. Then the tasks are bundled and sent to the cluster.\n",
    "\n",
    "**STEP 3**\n",
    "Now the driver talks to the cluster manager and negotiates the resources. Cluster manager launches executors in worker nodes on behalf of the driver. At this point, the driver will send the tasks to the executors based on data placement. When executors start, they register themselves with drivers. So, the driver will have a complete view of executors that are executing the task.\n",
    "\n",
    "**STEP 4**\n",
    "During the course of execution of tasks, driver program will monitor the set of executors that runs. Driver node also schedules future tasks based on data placement. \n",
    "\n",
    "![architecture](architecture.png \"Pyspark Architecture\")\n",
    "\n",
    "## PySpark Features\n",
    "- In-memory computation\n",
    "- Distributed processing using parallelize\n",
    "- Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)\n",
    "- Fault-tolerant\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Cache & persistence\n",
    "- Inbuild-optimization when using DataFrames\n",
    "- Supports ANSI SQL\n",
    "\n",
    "## Advantages of PySpark\n",
    "- PySpark is a general-purpose, in-memory, distributed processing engine that allows you to process data efficiently in a distributed fashion.\n",
    "- Applications running on PySpark are 100x faster than traditional systems.\n",
    "- You will get great benefits using PySpark for data ingestion pipelines.\n",
    "- Using PySpark we can process data from Hadoop HDFS, AWS S3, and many file systems.\n",
    "- PySpark also is used to process real-time data using Streaming and Kafka.\n",
    "- Using PySpark streaming you can also stream files from the file system and also stream from the socket.\n",
    "- PySpark natively has machine learning and graph libraries.\n",
    "\n",
    "## Installation Guide\n",
    "For installing PySpark, Follow the guidelines present on the official PySpark page,\n",
    "\n",
    "- [PySpark Installation Guide](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "*OR* \n",
    "\n",
    "You can use *Docker* to set up the environment using the following guide,\n",
    "\n",
    "- [PySpark using Docker](https://towardsdatascience.com/stuck-trying-to-get-pyspark-to-work-in-your-data-science-environment-here-is-another-way-fb80a4bb7d8f)\n",
    "\n",
    "## Dataset Used\n",
    "The Dataset used for this tutorial can be found at [here](https://www.kaggle.com/datasets/dinnymathew/usstockprices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da7ff2-5342-404b-a400-00040b06aca1",
   "metadata": {},
   "source": [
    "# PySpark Code Samples\n",
    "\n",
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee08036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9bb969-be92-4d4c-b722-5ffc8dfb6848",
   "metadata": {},
   "source": [
    "## Creating Spark App\n",
    "\n",
    "The entry point into all functionality in Spark is the SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904d0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('Pyspark_Tut') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d940b",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view.\n",
    "\n",
    "### Reading csv file\n",
    "PySpark provides ```spark.read().csv(\"file_name\")``` to read a file or directory of files in CSV format into Spark DataFrame. Function `option()` can be used to customize the behavior of reading or writing, such as controlling behavior of the header, delimiter character, character set, and so on.\n",
    "\n",
    "The `printSchema()` method is used to display the schema of the PySpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca610bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- adjusted: string (nullable = true)\n",
      " |-- market.cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_data = spark.read.csv(\n",
    "    \"stocks_price_final.csv\",\n",
    "    sep=',',\n",
    "    header=True\n",
    ")\n",
    "\n",
    "s_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df07ca5-09cb-4ca5-aaa8-1c6d29a213cc",
   "metadata": {},
   "source": [
    "One of the greatest features of Apache Spark is its ability to infer the schema on the fly. By default, `inferSchema` is set to `False`, meaning no schema inference will be done from data.\n",
    "\n",
    "## Reading with infer schema\n",
    "\n",
    "When set to `True`, pyspark infers the input schema automatically from data. *It requires one extra pass over the data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce11a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- adjusted: string (nullable = true)\n",
      " |-- market.cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferSchema_data = spark.read.csv(\n",
    "    \"stocks_price_final.csv\",\n",
    "    sep=',',\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "\n",
    "inferSchema_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c225d7",
   "metadata": {},
   "source": [
    "### Define Schema\n",
    "\n",
    "PySpark provides the `StructType()` and `StructField()` methods which are used to define the columns in the PySpark DataFrame.\n",
    "\n",
    "Using these methods, we can define the column names and the data types of the particular columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42084a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "    StructField('_c0', IntegerType(), True),\n",
    "    StructField('symbol', StringType(), True),\n",
    "    StructField('data', DateType(), True),\n",
    "    StructField('open', DoubleType(), True),\n",
    "    StructField('high', DoubleType(), True),\n",
    "    StructField('low', DoubleType(), True),\n",
    "    StructField('close', DoubleType(), True),\n",
    "    StructField('volume', IntegerType(), True),\n",
    "    StructField('adjusted', DoubleType(), True),\n",
    "    StructField('market.cap', StringType(), True),\n",
    "    StructField('sector', StringType(), True),\n",
    "    StructField('industry', StringType(), True),\n",
    "    StructField('exchange', StringType(), True)\n",
    "]\n",
    "\n",
    "final_struc = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b766f6-c33e-4414-ac16-26fa5a467e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- adjusted: double (nullable = true)\n",
      " |-- market.cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\n",
    "    \"stocks_price_final.csv\",\n",
    "    sep=',',\n",
    "    header=True,\n",
    "    schema = final_struc\n",
    ")\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2384880-9c8f-4417-a9a5-1586f8477290",
   "metadata": {},
   "source": [
    "## Viewing Data\n",
    "\n",
    "### show method\n",
    "The top rows of a DataFrame can be displayed using `DataFrame.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc07bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5704e-d2c7-46ec-b7af-117a83696ce3",
   "metadata": {},
   "source": [
    "The rows can also be shown vertically. This is useful when rows are too long to show horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed286806-60ce-4e65-b3ed-e2b85554a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " _c0        | 1                    \n",
      " symbol     | TXG                  \n",
      " data       | 2019-09-12           \n",
      " open       | 54.0                 \n",
      " high       | 58.0                 \n",
      " low        | 51.0                 \n",
      " close      | 52.75                \n",
      " volume     | 7326300              \n",
      " adjusted   | 52.75                \n",
      " market.cap | $9.31B               \n",
      " sector     | Capital Goods        \n",
      " industry   | Biotechnology: La... \n",
      " exchange   | NASDAQ               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65daeb8a-49c4-4395-8297-b85a5fdb039f",
   "metadata": {},
   "source": [
    "### describe\n",
    "\n",
    "Show the summary of the DataFrame, use `describe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aab8848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|              open|             close|              high|               low|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|           1726301|           1726301|           1726301|           1726301|\n",
      "|   mean|15070.071703341047| 15032.71485433071|15555.067268137085|14557.808227578987|\n",
      "| stddev|1111821.8002863186|1109755.9294000594|1148247.1953514975|1072968.1558434179|\n",
      "|    min|             0.072|             0.071|             0.078|             0.052|\n",
      "|    max|      1.60168176E8|      1.58376592E8|      1.61601456E8|      1.55151728E8|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"open\", \"close\", \"high\", \"low\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827dd7d-e935-4737-92f0-1b2ef0c75b03",
   "metadata": {},
   "source": [
    "### Collect data\n",
    "\n",
    "`DataFrame.collect()` collects the distributed data to the driver side as the local data in Python. \n",
    "\n",
    "**Note** that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.\n",
    "\n",
    "In order to avoid throwing an out-of-memory exception, use `DataFrame.take()` or `DataFrame.tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402fc821-7a5a-4f63-949b-43a48be172a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, symbol='TXG', data=datetime.date(2019, 9, 12), open=54.0, high=58.0, low=51.0, close=52.75, volume=7326300, adjusted=52.75, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=2, symbol='TXG', data=datetime.date(2019, 9, 13), open=52.75, high=54.355, low=49.150002, close=52.27, volume=1025200, adjusted=52.27, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=3, symbol='TXG', data=datetime.date(2019, 9, 16), open=52.450001, high=56.0, low=52.009998, close=55.200001, volume=269900, adjusted=55.200001, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=4, symbol='TXG', data=datetime.date(2019, 9, 17), open=56.209999, high=60.900002, low=55.423, close=56.779999, volume=602800, adjusted=56.779999, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
       " Row(_c0=5, symbol='TXG', data=datetime.date(2019, 9, 18), open=56.849998, high=62.27, low=55.650002, close=62.0, volume=1589600, adjusted=62.0, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_data = data.collect()  # ---- May through memory error\n",
    "\n",
    "sub_data = data.take(5)\n",
    "sub_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2769134-6685-4018-816e-daa9863ddd7a",
   "metadata": {},
   "source": [
    "### To Pandas\n",
    "\n",
    "PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aef07c7-a94d-4bc9-ba93-15d25e88fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>symbol</th>\n",
       "      <th>data</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjusted</th>\n",
       "      <th>market.cap</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>exchange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TXG</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>52.750000</td>\n",
       "      <td>7326300.0</td>\n",
       "      <td>52.750000</td>\n",
       "      <td>$9.31B</td>\n",
       "      <td>Capital Goods</td>\n",
       "      <td>Biotechnology: Laboratory Analytical Instruments</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TXG</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>52.750000</td>\n",
       "      <td>54.355000</td>\n",
       "      <td>49.150002</td>\n",
       "      <td>52.270000</td>\n",
       "      <td>1025200.0</td>\n",
       "      <td>52.270000</td>\n",
       "      <td>$9.31B</td>\n",
       "      <td>Capital Goods</td>\n",
       "      <td>Biotechnology: Laboratory Analytical Instruments</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TXG</td>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>52.450001</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>52.009998</td>\n",
       "      <td>55.200001</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>55.200001</td>\n",
       "      <td>$9.31B</td>\n",
       "      <td>Capital Goods</td>\n",
       "      <td>Biotechnology: Laboratory Analytical Instruments</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TXG</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>56.209999</td>\n",
       "      <td>60.900002</td>\n",
       "      <td>55.423000</td>\n",
       "      <td>56.779999</td>\n",
       "      <td>602800.0</td>\n",
       "      <td>56.779999</td>\n",
       "      <td>$9.31B</td>\n",
       "      <td>Capital Goods</td>\n",
       "      <td>Biotechnology: Laboratory Analytical Instruments</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TXG</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>56.849998</td>\n",
       "      <td>62.270000</td>\n",
       "      <td>55.650002</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1589600.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>$9.31B</td>\n",
       "      <td>Capital Goods</td>\n",
       "      <td>Biotechnology: Laboratory Analytical Instruments</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0 symbol        data       open       high        low      close  \\\n",
       "0    1    TXG  2019-09-12  54.000000  58.000000  51.000000  52.750000   \n",
       "1    2    TXG  2019-09-13  52.750000  54.355000  49.150002  52.270000   \n",
       "2    3    TXG  2019-09-16  52.450001  56.000000  52.009998  55.200001   \n",
       "3    4    TXG  2019-09-17  56.209999  60.900002  55.423000  56.779999   \n",
       "4    5    TXG  2019-09-18  56.849998  62.270000  55.650002  62.000000   \n",
       "\n",
       "      volume   adjusted market.cap         sector  \\\n",
       "0  7326300.0  52.750000     $9.31B  Capital Goods   \n",
       "1  1025200.0  52.270000     $9.31B  Capital Goods   \n",
       "2   269900.0  55.200001     $9.31B  Capital Goods   \n",
       "3   602800.0  56.779999     $9.31B  Capital Goods   \n",
       "4  1589600.0  62.000000     $9.31B  Capital Goods   \n",
       "\n",
       "                                           industry exchange  \n",
       "0  Biotechnology: Laboratory Analytical Instruments   NASDAQ  \n",
       "1  Biotechnology: Laboratory Analytical Instruments   NASDAQ  \n",
       "2  Biotechnology: Laboratory Analytical Instruments   NASDAQ  \n",
       "3  Biotechnology: Laboratory Analytical Instruments   NASDAQ  \n",
       "4  Biotechnology: Laboratory Analytical Instruments   NASDAQ  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data = data.toPandas()\n",
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cbfe6-5ec2-4e4a-9935-0e18f581a95f",
   "metadata": {},
   "source": [
    "# Basic Operations\n",
    "\n",
    "## Creating new Column\n",
    "\n",
    "Creating a new column from an existing column can done using the `withColumn` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b556c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+----------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|      date|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+----------+\n",
      "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|2019-09-12|\n",
      "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|2019-09-13|\n",
      "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|2019-09-16|\n",
      "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|2019-09-17|\n",
      "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|2019-09-18|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('date', data.data)\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac6aa5-1724-4b1f-96e6-b7f854d28d08",
   "metadata": {},
   "source": [
    "### Column Renaming\n",
    "\n",
    "Renaming a column a=can be achieved using `withColumnRenamed` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "170b27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+------------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|data_changed|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+------------+\n",
      "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|  2019-09-12|\n",
      "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|  2019-09-13|\n",
      "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|  2019-09-16|\n",
      "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|  2019-09-17|\n",
      "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|  2019-09-18|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed('date', 'data_changed')\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd2da0-9def-4015-b1a8-d78487f789b6",
   "metadata": {},
   "source": [
    "### Drop Column\n",
    "\n",
    "Dropping a column a=can be achieved using `drop` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11ccb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.drop('data_changed')\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9870be8-7bca-4ec8-8b20-f39be9a9a70f",
   "metadata": {},
   "source": [
    "## Subsetting Data\n",
    "\n",
    "Select only columns needed from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b11ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+-------+---------+\n",
      "|     open|     high|      low|    close| volume| adjusted|\n",
      "+---------+---------+---------+---------+-------+---------+\n",
      "|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|\n",
      "|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|\n",
      "|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|\n",
      "|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|\n",
      "|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|\n",
      "+---------+---------+---------+---------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(['open', 'high', 'low', 'close', 'volume', 'adjusted']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8ea8a-3a5b-497f-bb96-0eb118ee6f0d",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "To select a subset of rows, use `DataFrame.filter()` or `Dataframe.where()`.\n",
    "\n",
    "\n",
    "Selecting Data for **Health Care** sector only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2d5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+----+----+-----+------+--------+----------+-----------+--------------------+--------+\n",
      "|_c0|symbol|      data|open|high| low|close|volume|adjusted|market.cap|     sector|            industry|exchange|\n",
      "+---+------+----------+----+----+----+-----+------+--------+----------+-----------+--------------------+--------+\n",
      "|218|    YI|2019-01-02|6.02|6.11| 6.0|  6.0|  4100|     6.0|  $560.04M|Health Care|Medical/Nursing S...|  NASDAQ|\n",
      "|219|    YI|2019-01-03|6.02|6.05|5.95| 6.05|  4700|    6.05|  $560.04M|Health Care|Medical/Nursing S...|  NASDAQ|\n",
      "|220|    YI|2019-01-04|6.05|6.97|6.05| 6.78| 10300|    6.78|  $560.04M|Health Care|Medical/Nursing S...|  NASDAQ|\n",
      "|221|    YI|2019-01-07| 6.8| 7.0|6.55|  7.0| 10100|     7.0|  $560.04M|Health Care|Medical/Nursing S...|  NASDAQ|\n",
      "|222|    YI|2019-01-08| 7.0|9.76| 6.9| 9.55| 60300|    9.55|  $560.04M|Health Care|Medical/Nursing S...|  NASDAQ|\n",
      "+---+------+----------+----+----+----+-----+------+--------+----------+-----------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "health = data.filter(col('sector') == 'Health Care')\n",
    "\n",
    "health.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fb670-9948-4de7-bccb-5451098d981d",
   "metadata": {},
   "source": [
    "Selecting `[date, open, close, adjusted]` columns for **Technology** sector only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f606f01d-82a9-48cc-ae23-c9735f004e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+--------+\n",
      "|      data|open|close|adjusted|\n",
      "+----------+----+-----+--------+\n",
      "|2019-01-02|8.51| 8.55|    8.55|\n",
      "|2019-01-03| 8.5| 8.59|    8.59|\n",
      "|2019-01-04|8.72| 8.88|    8.88|\n",
      "|2019-01-07|8.88| 8.86|    8.86|\n",
      "|2019-01-08|8.93|  9.4|     9.4|\n",
      "+----------+----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tech = data.where(col('sector') == 'Technology').select('data', 'open', 'close', 'adjusted')\n",
    "\n",
    "tech.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63e747-3395-48f6-840d-ae88ff622ec2",
   "metadata": {},
   "source": [
    "- `lit()` - PySpark SQL function lit() are used to add a new column to DataFrame by assigning a literal or constant value\n",
    "- `col()` - Returns a Column based on the given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7f497c-2dcb-463d-b32b-b96ddabfebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+------+---------+----------+-------------+--------------------+--------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close|volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+---+------+----------+---------+---------+---------+---------+------+---------+----------+-------------+--------------------+--------+\n",
      "| 78|   TXG|2020-01-02|76.910004|77.989998|71.480003|72.830002|220200|72.830002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 79|   TXG|2020-01-03|71.519997|76.188004|70.580002|75.559998|288300|75.559998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 80|   TXG|2020-01-06|75.269997|77.349998|73.559998|75.550003|220600|75.550003|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 81|   TXG|2020-01-07|     76.0|77.279999|    75.32|75.980003|182400|75.980003|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 82|   TXG|2020-01-08|76.089996|76.949997|72.739998|74.839996|172100|74.839996|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+---+------+----------+---------+---------+---------+---------+------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter( (col('data') >= lit('2020-01-01')) & (col('data') <= lit('2020-01-31')) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e9486-38a3-4817-8658-96a123bcae55",
   "metadata": {},
   "source": [
    "### Range Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3e3437-88d9-45a5-a0c6-8883f478cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+----------+----------+----------+----------+------+----------+----------+-------------+--------------------+--------+\n",
      "| _c0|symbol|      data|      open|      high|       low|     close|volume|  adjusted|market.cap|       sector|            industry|exchange|\n",
      "+----+------+----------+----------+----------+----------+----------+------+----------+----------+-------------+--------------------+--------+\n",
      "|  93|   TXG|2020-01-24| 95.459999|     101.0| 94.157997|100.790001|328100|100.790001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  94|   TXG|2020-01-27| 99.760002|104.892998| 97.019997|103.209999|334900|103.209999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  95|   TXG|2020-01-28|104.620003|108.269997|103.297997|106.620003|245400|106.620003|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|6893|  ABMD|2019-01-02|315.940002|320.709991|307.029999|309.959991|590000|309.959991|   $13.39B|  Health Care|Medical/Dental In...|  NASDAQ|\n",
      "|6894|  ABMD|2019-01-03|    307.25| 311.73999|293.660004|302.290009|665300|302.290009|   $13.39B|  Health Care|Medical/Dental In...|  NASDAQ|\n",
      "+----+------+----------+----------+----------+----------+----------+------+----------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(data.adjusted.between(100.0, 500.0)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d8907-f5f4-48d9-971f-41aa78435c33",
   "metadata": {},
   "source": [
    "### Case Statement\n",
    "\n",
    "You can perform a SQL style CASE statement using `when` and `otherwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e61ff9be-eb5e-42bc-984e-d0d13fdf96c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------------------------------+\n",
      "|     open|    close|CASE WHEN (close >= 60.0) THEN 1 ELSE 0 END|\n",
      "+---------+---------+-------------------------------------------+\n",
      "|     54.0|    52.75|                                          0|\n",
      "|    52.75|    52.27|                                          0|\n",
      "|52.450001|55.200001|                                          0|\n",
      "|56.209999|56.779999|                                          0|\n",
      "|56.849998|     62.0|                                          1|\n",
      "|62.810001|61.119999|                                          1|\n",
      "|61.709999|     60.5|                                          1|\n",
      "|60.220001|60.330002|                                          1|\n",
      "|     61.0|54.299999|                                          0|\n",
      "|54.459999|52.759998|                                          0|\n",
      "+---------+---------+-------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('open', 'close', when(data.close >= 60.0, 1).otherwise(0)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d979bae-36eb-4b3e-b612-25aab5c05ea6",
   "metadata": {},
   "source": [
    "### Regex Matching using `rlike`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8747f041-5b94-40ae-bcf1-b3b7d7feee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|              sector|Sector Starting with B or C|\n",
      "+--------------------+---------------------------+\n",
      "|         Health Care|                      false|\n",
      "|       Capital Goods|                       true|\n",
      "|Consumer Non-Dura...|                       true|\n",
      "|    Public Utilities|                      false|\n",
      "|   Consumer Durables|                       true|\n",
      "|             Finance|                      false|\n",
      "|      Transportation|                      false|\n",
      "|       Miscellaneous|                      false|\n",
      "|   Consumer Services|                       true|\n",
      "|              Energy|                      false|\n",
      "|    Basic Industries|                       true|\n",
      "|          Technology|                      false|\n",
      "+--------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('sector', \n",
    "            data.sector.rlike('^[B,C]').alias('Sector Starting with B or C')\n",
    "            ).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc85fe-feb7-4349-8b29-eff0bb4ea4b0",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame.\n",
    "\n",
    "Available Aggregations:\n",
    "\n",
    "| Function | Description |\n",
    "| --- | --- |\n",
    "| `count()` | Returns the count of rows for each group. |\n",
    "| `mean()` | Returns the mean of values for each group. |\n",
    "| `max()` | Returns the maximum of values for each group. |\n",
    "| `min()` | Returns the minimum of values for each group. |\n",
    "| `sum()` | Returns the total for values for each group. | \n",
    "| `avg()` | Returns the average for values for each group. |\n",
    "| `agg()` | Using agg() function, we can calculate more than one aggregate at a time. |\n",
    "| `pivot()` | This function is used to Pivot the DataFrame |\n",
    "\n",
    "## Count\n",
    "\n",
    "Counts the number of records in a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64ffe9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              sector| count|\n",
      "+--------------------+------+\n",
      "|       Miscellaneous| 50221|\n",
      "|         Health Care|316175|\n",
      "|    Public Utilities| 72836|\n",
      "|              Energy| 87494|\n",
      "|Consumer Non-Dura...| 78080|\n",
      "|             Finance|303180|\n",
      "|    Basic Industries| 97323|\n",
      "|       Capital Goods|133122|\n",
      "|          Technology|229799|\n",
      "|   Consumer Services|272393|\n",
      "|   Consumer Durables| 48404|\n",
      "|      Transportation| 40007|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('sector').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173022-60f2-4ab3-ac02-0cca5e8388a9",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0a4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+\n",
      "|              sector|         avg(open)|        avg(close)|     avg(adjusted)|\n",
      "+--------------------+------------------+------------------+------------------+\n",
      "|       Miscellaneous| 52.03839496900667| 52.06362854950963| 51.80973033632302|\n",
      "|         Health Care|119.96763306523248|119.07806125419023|118.97394778016215|\n",
      "|    Public Utilities|  35.5807773523944| 35.58528245861939| 34.73015568500465|\n",
      "|              Energy| 24.45658989126103|24.427350302157844|23.684714263000885|\n",
      "|Consumer Non-Dura...| 43.32860274612681|43.330386013645914| 42.81762456569027|\n",
      "|             Finance|37.774667068190475|37.779002314288824| 37.10028522718037|\n",
      "|    Basic Industries|266410.35470107093| 265750.3613671152|263865.51070311887|\n",
      "|       Capital Goods| 60.48854363282767| 60.51655483568793| 59.97512253879296|\n",
      "|          Technology|49.516045118394906| 49.53479888748223| 49.25234033754494|\n",
      "|   Consumer Services| 55.07886734259143|55.055247536473516| 54.36187827846141|\n",
      "|   Consumer Durables|391.03153998498055| 390.1041550657182| 389.6417639493618|\n",
      "|      Transportation| 37.30503242702783| 37.29532668104834| 36.80231922791236|\n",
      "+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(['sector', 'open', 'close', 'adjusted']).groupBy('sector').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd988ad-3e5f-4c74-b7ea-a9d913c9b6f3",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "\n",
    "Using `agg()` function, we can calculate more than one aggregate at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3da9b567-22a8-4749-9e8e-781614bb784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+----------+---------------+--------------+------------------+---------------+---------------+------------------+\n",
      "|sector               |From      |To        |Minimum Opening|Maxmum Opening|Average Opening   |Minimum Closing|Maximum Closing|Average Closing   |\n",
      "+---------------------+----------+----------+---------------+--------------+------------------+---------------+---------------+------------------+\n",
      "|Miscellaneous        |2019-01-02|2020-07-22|0.147          |1059.98999    |52.03839496900667 |0.1361         |1035.829956    |52.06362854950963 |\n",
      "|Health Care          |2019-01-02|2020-07-22|0.072          |186000.0      |119.96763306523248|0.071          |187000.0       |119.07806125419023|\n",
      "|Public Utilities     |2019-01-02|2020-07-22|0.331          |280.0         |35.5807773523944  |0.325          |282.220001     |35.58528245861939 |\n",
      "|Energy               |2019-01-02|2020-07-22|0.1            |905.0         |24.45658989126103 |0.09           |901.039978     |24.427350302157844|\n",
      "|Consumer Non-Durables|2019-01-02|2020-07-22|0.12           |655.0         |43.32860274612681 |0.12           |664.130005     |43.330386013645914|\n",
      "|Finance              |2019-01-02|2020-07-22|0.25           |1336.930054   |37.774667068190475|0.27           |1341.079956    |37.779002314288824|\n",
      "|Basic Industries     |2019-01-02|2020-07-22|0.23           |1.60168176E8  |266410.35470107093|0.23           |1.58376592E8   |265750.3613671152 |\n",
      "|Capital Goods        |2019-01-02|2020-07-22|0.13           |4025.0        |60.48854363282767 |0.12           |4037.77002     |60.51655483568793 |\n",
      "|Technology           |2019-01-02|2020-07-22|0.14           |2704.0        |49.516045118394906|0.13           |2736.0         |49.53479888748223 |\n",
      "|Consumer Services    |2019-01-02|2020-07-22|0.1            |15437.5       |55.07886734259143 |0.134          |19843.75       |55.055247536473516|\n",
      "|Consumer Durables    |2019-01-02|2020-07-22|0.32           |111718.75     |391.03153998498055|0.31           |118750.0       |390.1041550657182 |\n",
      "|Transportation       |2019-01-02|2020-07-22|0.08           |274.410004    |37.30503242702783 |0.08           |274.040009     |37.29532668104834 |\n",
      "+---------------------+----------+----------+---------------+--------------+------------------+---------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('sector').agg(\n",
    "    min(\"data\").alias(\"From\"),\n",
    "    max(\"data\").alias(\"To\"),\n",
    "    min(\"open\").alias(\"Minimum Opening\"),\n",
    "    max(\"open\").alias(\"Maxmum Opening\"),\n",
    "    avg(\"open\").alias(\"Average Opening\"),\n",
    "    min(\"close\").alias(\"Minimum Closing\"),\n",
    "    max(\"close\").alias(\"Maximum Closing\"),\n",
    "    avg(\"close\").alias(\"Average Closing\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7d3e1-ddee-43b9-a17d-665630d39fbe",
   "metadata": {},
   "source": [
    "## Pivot\n",
    "\n",
    "Pivot() is an aggregation where one of the grouping columns values transposed into individual columns with distinct data.\n",
    "\n",
    "Aggregating data for each `industry` for each `exhange` by sum of `open` in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24e8335a-bca6-40aa-9f29-6de9bc9050ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            industry|              NASDAQ|              NYSE|\n",
      "+--------------------+--------------------+------------------+\n",
      "|    Ophthalmic Goods|  24940.150004000003|137920.41975700003|\n",
      "|Biotechnology: Bi...|  1080056.3640190002|14947.620007000003|\n",
      "|Biotechnology: El...|  210061.58897299992|122226.08995200001|\n",
      "|Precision Instrum...|   9594.619975999998|              null|\n",
      "|Medical/Nursing S...|  197619.64422999995|265969.70723499986|\n",
      "|Hospital/Nursing ...|   61067.73431699995|156129.72005899984|\n",
      "|Medical Specialities|  191181.31875899993| 602157.3348899995|\n",
      "|Biotechnology: In...|  246407.79592300003|              null|\n",
      "|Misc Health and B...|  15340.322973000004|43541.455038000044|\n",
      "| Medical Electronics|  19621.877031000004|              null|\n",
      "|Other Pharmaceuti...|  12111.344007999998|140996.56995299988|\n",
      "|Medical/Dental In...|  1165266.6330440005| 767942.2839969998|\n",
      "|Industrial Specia...|  366984.73284199974| 209960.1032380001|\n",
      "|Major Pharmaceuti...|3.0843949254545037E7| 583502.8204350004|\n",
      "|Biotechnology: Co...|   261449.8164530002|112037.35006000003|\n",
      "+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "health.groupBy('industry').pivot('exchange').sum('open').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1b4a3-8e7d-4535-910f-464e688f0d8e",
   "metadata": {},
   "source": [
    "# Ordering, Duplicate and Null Handling\n",
    "\n",
    "## Duplicate Handling\n",
    "\n",
    "### distinct\n",
    "\n",
    "PySpark `distinct()` function is used to drop/remove the duplicate rows (all columns) from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f305ee-1041-45ec-9d66-491aac927d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 1729034\n",
      "+----+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "| _c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+----+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "| 208|   TXG|2020-07-09|     92.0|92.400002|89.660004|92.290001| 447100|92.290001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 216|   TXG|2020-07-21|93.099998|95.269997|91.860001|95.209999|2306800|95.209999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "| 703|   PIH|2019-05-16|     5.44|     5.58|      5.4|     5.55|  14700|     5.55|   $27.43M|      Finance|Property-Casualty...|  NASDAQ|\n",
      "| 761|   PIH|2019-08-08|     4.91|      5.2|     4.88|     5.02|   6600|     5.02|   $27.43M|      Finance|Property-Casualty...|  NASDAQ|\n",
      "|1002|  TURN|2019-01-02|     1.74|      1.8|     1.73|      1.8|  27500|      1.8|   $54.46M|      Finance|Finance/Investors...|  NASDAQ|\n",
      "+----+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinctdf = data.distinct()\n",
    "print(\"Distinct count: \"+str(distinctdf.count()))\n",
    "distinctdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51591575-c658-442c-95f3-53fa9321b4e1",
   "metadata": {},
   "source": [
    "### dropDuplicates()\n",
    "\n",
    "`dropDuplicates()` is used to drop rows based on selected (one or multiple) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8987c7ab-2643-4a5f-b5f1-9ee0375ac78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of sector and industry : 208\n",
      "+------+------+----------+---------+-----+---------+---------+-------+---------+----------+----------------+--------------------+--------+\n",
      "|   _c0|symbol|      data|     open| high|      low|    close| volume| adjusted|market.cap|          sector|            industry|exchange|\n",
      "+------+------+----------+---------+-----+---------+---------+-------+---------+----------+----------------+--------------------+--------+\n",
      "|164990|  CLXT|2019-01-02|    10.27|10.67|    10.07|    10.39|  40700|    10.39|  $156.61M|Basic Industries|Agricultural Chem...|  NASDAQ|\n",
      "|189134|  CENX|2019-01-02|     7.18| 7.85|     6.98|     7.55|1828900|     7.55|  $689.74M|Basic Industries|            Aluminum|  NASDAQ|\n",
      "|990154|  YTEN|2019-01-02|34.799999| 36.0|30.799999|30.799999|   1300|30.799999|   $13.46M|Basic Industries|Containers/Packaging|  NASDAQ|\n",
      "|213934|  CLSK|2019-01-02|     20.0| 24.0|     20.0|22.799999|  10737|22.799999|   $52.09M|Basic Industries|Electric Utilitie...|  NASDAQ|\n",
      "|225885|   HHT|2019-01-02|     2.58| 2.88|      2.2|     2.88|  71600|     2.88|   $39.46M|Basic Industries|Engineering & Con...|  NASDAQ|\n",
      "+------+------+----------+---------+-----+---------+---------+-------+---------+----------+----------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropDisDF = data.dropDuplicates([\"sector\",\"industry\"])\n",
    "print(\"Distinct count of sector and industry : \"+str(dropDisDF.count()))\n",
    "dropDisDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c51a59-cb76-4628-a5d8-bb4e160e342f",
   "metadata": {},
   "source": [
    "## Null Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deb9aaf1-6b43-4956-90e0-71323c3d0b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----+--------+------+--------+\n",
      "|sector|industry|open|close|adjusted|volume|exchange|\n",
      "+------+--------+----+-----+--------+------+--------+\n",
      "|     0|       0|2733| 2733|    2733|  3827|       0|\n",
      "+------+--------+----+-----+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sub = data.select('sector', 'industry', 'open', 'close', 'adjusted', 'volume', 'exchange')\n",
    "\n",
    "data_sub.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data_sub.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b053ac70-9b6f-4d0e-ae92-0924e16d12df",
   "metadata": {},
   "source": [
    "### Dropping if any column contains null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "299cf336-6e6f-4014-a4fa-817d9f2bf69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Drop count: 1729034\n",
      "Before Drop count: 1725207\n",
      "Dropped count: 3827\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before Drop count: {data_sub.count()}\")\n",
    "df = data_sub.na.drop()\n",
    "print(f\"Before Drop count: {df.count()}\")\n",
    "print(f\"Dropped count: {data_sub.count() - df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226072ee-8222-48e8-a662-c9bdf173b412",
   "metadata": {},
   "source": [
    "### Dropping if subset of columns contains null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "637ba107-3b7e-4319-b351-c1ac0608a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Drop count: 1729034\n",
      "Before Drop count: 1726301\n",
      "Dropped count: 2733\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before Drop count: {data_sub.count()}\")\n",
    "df = data_sub.na.drop(subset=[\"open\", \"close\"])\n",
    "print(f\"Before Drop count: {df.count()}\")\n",
    "print(f\"Dropped count: {data_sub.count() - df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcc733-0109-4899-9e7f-41c5aee56849",
   "metadata": {},
   "source": [
    "### Filling null values with constant\n",
    "\n",
    "PySpark `fill(value:Long)` is used to replace NULL/None values with numeric values either zero(0) or any constant value for all integer and long datatype columns of PySpark DataFrame or Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98f7bee0-bf48-47c6-a42f-c13b5541f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-----+--------+\n",
      "|       sector|open|close|exchange|\n",
      "+-------------+----+-----+--------+\n",
      "|Miscellaneous| 0.0|  0.0|  NASDAQ|\n",
      "|  Health Care| 0.0|  0.0|  NASDAQ|\n",
      "|  Health Care| 0.0|  0.0|  NASDAQ|\n",
      "|  Health Care| 0.0|  0.0|  NASDAQ|\n",
      "|  Health Care| 0.0|  0.0|  NASDAQ|\n",
      "+-------------+----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data_sub.na.fill(value=0)\n",
    "df.select('sector', 'open', 'close', 'exchange').where(col(\"open\") == 0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d46d07-934e-4daa-95fe-5fb7a7fb1514",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ordering / Sorting\n",
    "\n",
    "You can use either `sort()` or `orderBy()` function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns, you can also do sorting using PySpark SQL sorting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2c637dc-94ba-443d-9166-6f7da77a148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+---------+\n",
      "| sector|            industry|     open|    close|\n",
      "+-------+--------------------+---------+---------+\n",
      "|Finance|Accident &Health ...|17.190001|    17.57|\n",
      "|Finance|Accident &Health ...|     20.1|    20.16|\n",
      "|Finance|Accident &Health ...|    17.49|17.139999|\n",
      "|Finance|Accident &Health ...|17.379999|    17.66|\n",
      "|Finance|Accident &Health ...|17.620001|     17.9|\n",
      "|Finance|Accident &Health ...|    18.26|18.139999|\n",
      "|Finance|Accident &Health ...|18.120001|18.540001|\n",
      "|Finance|Accident &Health ...|     18.4|17.530001|\n",
      "|Finance|Accident &Health ...|    17.41|    17.65|\n",
      "|Finance|Accident &Health ...|17.540001|17.450001|\n",
      "|Finance|Accident &Health ...|17.459999|    17.75|\n",
      "|Finance|Accident &Health ...|    17.82|    18.52|\n",
      "|Finance|Accident &Health ...|18.379999|    19.26|\n",
      "|Finance|Accident &Health ...|    19.32|19.620001|\n",
      "|Finance|Accident &Health ...|    19.41|    19.83|\n",
      "|Finance|Accident &Health ...|    19.83|19.790001|\n",
      "|Finance|Accident &Health ...|    19.73|19.790001|\n",
      "|Finance|Accident &Health ...|19.799999|19.870001|\n",
      "|Finance|Accident &Health ...|    19.77|    19.98|\n",
      "|Finance|Accident &Health ...|20.030001|    19.98|\n",
      "+-------+--------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort('industry') \\\n",
    "    .select('sector', 'industry', 'open', 'close') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7e5fc60-e753-490e-b354-98f1b4845049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+------------+------------+------------+------+--------+\n",
      "|          sector|       industry|        open|       close|    adjusted|volume|exchange|\n",
      "+----------------+---------------+------------+------------+------------+------+--------+\n",
      "|Basic Industries|Major Chemicals|1.60168176E8|1.58376592E8|1.57249392E8|     0|    NYSE|\n",
      "|Basic Industries|Major Chemicals|1.59451552E8|1.56943312E8|1.55826304E8|     0|    NYSE|\n",
      "|Basic Industries|Major Chemicals|1.57659952E8|1.45835456E8|1.44797504E8|     0|    NYSE|\n",
      "|Basic Industries|Major Chemicals|  1.504936E8|1.44760512E8|1.43731152E8|     0|    NYSE|\n",
      "|Basic Industries|Major Chemicals|1.46193776E8|1.40819008E8|1.39816768E8|     0|    NYSE|\n",
      "+----------------+---------------+------------+------------+------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"open\").desc(),col(\"sector\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa8104-b878-49a3-bd4c-c5249f9dd00e",
   "metadata": {},
   "source": [
    "# Joins\n",
    "\n",
    "PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames; it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN.\n",
    "\n",
    "| Join String | Equivalent SQL Join |\n",
    "| --- | --- |\n",
    "| inner | INNER JOIN |\n",
    "| outer, full, fullouter, full_outer | FULL OUTER JOIN |\n",
    "|left, leftouter, left_outer | LEFT JOIN |\n",
    "|right, rightouter, right_outer | RIGHT JOIN |\n",
    "|cross | |\n",
    "|anti, leftanti, left_anti | |\n",
    "|semi, leftsemi, left_semi | |\n",
    "\n",
    "## Setting up Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41fc1ea5-0359-481b-ac56-e79b7a11b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      "\n",
      "+----------+----------+-----------+-------+-----------+\n",
      "|product_id|name      |category   |price  |seller_city|\n",
      "+----------+----------+-----------+-------+-----------+\n",
      "|101       |Watch     |Fashion    |299.0  |Delhi      |\n",
      "|102       |Bag       |Fashion    |1350.0 |Mumbai     |\n",
      "|103       |Shoes     |Fashion    |2999.0 |Chennai    |\n",
      "|104       |Smartphone|Electronics|14999.0|Kolkata    |\n",
      "|105       |Books     |Study      |145.0  |Delhi      |\n",
      "|106       |Oil       |Grocery    |110.0  |Chennai    |\n",
      "|107       |Laptop    |Electronics|79999.0|Bengalore  |\n",
      "+----------+----------+-----------+-------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- purchased_product: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+---+-------+---+----------+-----------------+---------+\n",
      "|id |name   |age|product_id|purchased_product|city     |\n",
      "+---+-------+---+----------+-----------------+---------+\n",
      "|1  |Olivia |20 |101       |Watch            |Mumbai   |\n",
      "|2  |Aditya |25 |0         |NA               |Delhi    |\n",
      "|3  |Cory   |15 |106       |Oil              |Bengalore|\n",
      "|4  |Isabell|10 |0         |NA               |Chennai  |\n",
      "|5  |Dominic|30 |103       |Shoes            |Chennai  |\n",
      "|6  |Tyler  |65 |104       |Smartphone       |Delhi    |\n",
      "|7  |Samuel |35 |0         |NA               |Kolkata  |\n",
      "|8  |Daniel |18 |0         |NA               |Delhi    |\n",
      "|9  |Jeremy |23 |107       |Laptop           |Mumbai   |\n",
      "+---+-------+---+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product = [(101,\"Watch\",\"Fashion\",299.0,\"Delhi\"), \\\n",
    "        (102,\"Bag\",\"Fashion\",1350.0,\"Mumbai\"), \\\n",
    "        (103,\"Shoes\",\"Fashion\",2999.0,\"Chennai\"), \\\n",
    "        (104,\"Smartphone\",\"Electronics\",14999.0,\"Kolkata\"), \\\n",
    "        (105,\"Books\",\"Study\",145.0,\"Delhi\"), \\\n",
    "        (106,\"Oil\",\"Grocery\",110.0,\"Chennai\"), \\\n",
    "        (107,\"Laptop\",\"Electronics\",79999.0,\"Bengalore\") \\\n",
    "]\n",
    "productColumns = [\"product_id\",\"name\",\"category\",\"price\", \"seller_city\"]\n",
    "\n",
    "productDF = spark.createDataFrame(data=product, schema = productColumns)\n",
    "productDF.printSchema()\n",
    "productDF.show(truncate=False)\n",
    "\n",
    "customer = [(1, \"Olivia\", 20, 101, \"Watch\", \"Mumbai\"), \\\n",
    "            (2, \"Aditya\", 25, 0, \"NA\", \"Delhi\"), \\\n",
    "            (3, \"Cory\", 15, 106, \"Oil\", \"Bengalore\"), \\\n",
    "            (4, \"Isabell\", 10, 0, \"NA\", \"Chennai\"), \\\n",
    "            (5, \"Dominic\", 30, 103, \"Shoes\", \"Chennai\"), \\\n",
    "            (6, \"Tyler\", 65, 104, \"Smartphone\", \"Delhi\"), \\\n",
    "            (7, \"Samuel\", 35, 0, \"NA\", \"Kolkata\"), \\\n",
    "            (8, \"Daniel\", 18, 0, \"NA\", \"Delhi\"), \\\n",
    "            (9, \"Jeremy\", 23, 107, \"Laptop\", \"Mumbai\"), \\\n",
    "]\n",
    "customerColumns = [\"id\",\"name\", \"age\", \"product_id\", \"purchased_product\", \"city\"]\n",
    "customerDF = spark.createDataFrame(data=customer, schema = customerColumns)\n",
    "customerDF.printSchema()\n",
    "customerDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c4ff6-a5e9-4b07-88b6-ccf3e7183351",
   "metadata": {},
   "source": [
    "## Inner Join\n",
    "\n",
    "This joins two datasets on key columns, where keys dont match the rows get dropped from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fbcabde-ca33-4925-8316-5f9096f8ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-------+-----------+---+-------+---+----------+-----------------+---------+\n",
      "|product_id|      name|   category|  price|seller_city| id|   name|age|product_id|purchased_product|     city|\n",
      "+----------+----------+-----------+-------+-----------+---+-------+---+----------+-----------------+---------+\n",
      "|       101|     Watch|    Fashion|  299.0|      Delhi|  1| Olivia| 20|       101|            Watch|   Mumbai|\n",
      "|       103|     Shoes|    Fashion| 2999.0|    Chennai|  5|Dominic| 30|       103|            Shoes|  Chennai|\n",
      "|       104|Smartphone|Electronics|14999.0|    Kolkata|  6|  Tyler| 65|       104|       Smartphone|    Delhi|\n",
      "|       106|       Oil|    Grocery|  110.0|    Chennai|  3|   Cory| 15|       106|              Oil|Bengalore|\n",
      "|       107|    Laptop|Electronics|79999.0|  Bengalore|  9| Jeremy| 23|       107|           Laptop|   Mumbai|\n",
      "+----------+----------+-----------+-------+-----------+---+-------+---+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "innerDF = productDF.join(customerDF, productDF.product_id == customerDF.product_id, \"inner\")\n",
    "innerDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb2600-ce66-47bf-82f0-067b5c5f4191",
   "metadata": {},
   "source": [
    "## Outer Join\n",
    "\n",
    "Outer a.k.a full, fullouter join returns all rows from both datasets, where join expression doesnt match it returns null on respective record columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f425861-3471-4b5f-a312-c9b66a7343a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-------+-----------+----+-------+----+----------+-----------------+---------+\n",
      "|product_id|      name|   category|  price|seller_city|  id|   name| age|product_id|purchased_product|     city|\n",
      "+----------+----------+-----------+-------+-----------+----+-------+----+----------+-----------------+---------+\n",
      "|      null|      null|       null|   null|       null|   2| Aditya|  25|         0|               NA|    Delhi|\n",
      "|      null|      null|       null|   null|       null|   4|Isabell|  10|         0|               NA|  Chennai|\n",
      "|      null|      null|       null|   null|       null|   7| Samuel|  35|         0|               NA|  Kolkata|\n",
      "|      null|      null|       null|   null|       null|   8| Daniel|  18|         0|               NA|    Delhi|\n",
      "|       101|     Watch|    Fashion|  299.0|      Delhi|   1| Olivia|  20|       101|            Watch|   Mumbai|\n",
      "|       102|       Bag|    Fashion| 1350.0|     Mumbai|null|   null|null|      null|             null|     null|\n",
      "|       103|     Shoes|    Fashion| 2999.0|    Chennai|   5|Dominic|  30|       103|            Shoes|  Chennai|\n",
      "|       104|Smartphone|Electronics|14999.0|    Kolkata|   6|  Tyler|  65|       104|       Smartphone|    Delhi|\n",
      "|       105|     Books|      Study|  145.0|      Delhi|null|   null|null|      null|             null|     null|\n",
      "|       106|       Oil|    Grocery|  110.0|    Chennai|   3|   Cory|  15|       106|              Oil|Bengalore|\n",
      "|       107|    Laptop|Electronics|79999.0|  Bengalore|   9| Jeremy|  23|       107|           Laptop|   Mumbai|\n",
      "+----------+----------+-----------+-------+-----------+----+-------+----+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outerDF = productDF.join(customerDF, productDF.product_id == customerDF.product_id, \"outer\")\n",
    "outerDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a306e8-37c7-4a94-b772-b6824cd52de5",
   "metadata": {},
   "source": [
    "## Left Outer Join\n",
    "\n",
    "Left a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesnt match, it assigns null for that record and drops records from right where match not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "292f8ec0-6d76-4154-b2b7-56f23ed1aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-------+-----------+----+-------+----+----------+-----------------+---------+\n",
      "|product_id|      name|   category|  price|seller_city|  id|   name| age|product_id|purchased_product|     city|\n",
      "+----------+----------+-----------+-------+-----------+----+-------+----+----------+-----------------+---------+\n",
      "|       101|     Watch|    Fashion|  299.0|      Delhi|   1| Olivia|  20|       101|            Watch|   Mumbai|\n",
      "|       102|       Bag|    Fashion| 1350.0|     Mumbai|null|   null|null|      null|             null|     null|\n",
      "|       103|     Shoes|    Fashion| 2999.0|    Chennai|   5|Dominic|  30|       103|            Shoes|  Chennai|\n",
      "|       104|Smartphone|Electronics|14999.0|    Kolkata|   6|  Tyler|  65|       104|       Smartphone|    Delhi|\n",
      "|       105|     Books|      Study|  145.0|      Delhi|null|   null|null|      null|             null|     null|\n",
      "|       106|       Oil|    Grocery|  110.0|    Chennai|   3|   Cory|  15|       106|              Oil|Bengalore|\n",
      "|       107|    Laptop|Electronics|79999.0|  Bengalore|   9| Jeremy|  23|       107|           Laptop|   Mumbai|\n",
      "+----------+----------+-----------+-------+-----------+----+-------+----+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftouterDF = productDF.join(customerDF, productDF.product_id == customerDF.product_id, \"left\")\n",
    "leftouterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ade00-dc67-44c1-8a83-4be27fc788b7",
   "metadata": {},
   "source": [
    "## Right Outer Join\n",
    "\n",
    "Right a.k.a Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of math found on the left dataset, when join expression doesnt match, it assigns null for that record and drops records from left where match not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1fb407c-72cb-413e-93c1-173fb22b7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-------+-----------+---+-------+---+----------+-----------------+---------+\n",
      "|product_id|      name|   category|  price|seller_city| id|   name|age|product_id|purchased_product|     city|\n",
      "+----------+----------+-----------+-------+-----------+---+-------+---+----------+-----------------+---------+\n",
      "|       101|     Watch|    Fashion|  299.0|      Delhi|  1| Olivia| 20|       101|            Watch|   Mumbai|\n",
      "|      null|      null|       null|   null|       null|  2| Aditya| 25|         0|               NA|    Delhi|\n",
      "|       106|       Oil|    Grocery|  110.0|    Chennai|  3|   Cory| 15|       106|              Oil|Bengalore|\n",
      "|      null|      null|       null|   null|       null|  4|Isabell| 10|         0|               NA|  Chennai|\n",
      "|       103|     Shoes|    Fashion| 2999.0|    Chennai|  5|Dominic| 30|       103|            Shoes|  Chennai|\n",
      "|       104|Smartphone|Electronics|14999.0|    Kolkata|  6|  Tyler| 65|       104|       Smartphone|    Delhi|\n",
      "|      null|      null|       null|   null|       null|  7| Samuel| 35|         0|               NA|  Kolkata|\n",
      "|      null|      null|       null|   null|       null|  8| Daniel| 18|         0|               NA|    Delhi|\n",
      "|       107|    Laptop|Electronics|79999.0|  Bengalore|  9| Jeremy| 23|       107|           Laptop|   Mumbai|\n",
      "+----------+----------+-----------+-------+-----------+---+-------+---+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rightouterDF = productDF.join(customerDF, productDF.product_id == customerDF.product_id, \"right\")\n",
    "rightouterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee880f21-d52c-44ab-b4a6-f6e8f311e58e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Left Semi Join \n",
    "\n",
    "leftsemi join is similar to inner join difference being leftsemi join returns all columns from the left dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b27dfe2-dfd2-47ed-93be-5fd2ec0717f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-------+-----------+\n",
      "|product_id|      name|   category|  price|seller_city|\n",
      "+----------+----------+-----------+-------+-----------+\n",
      "|       101|     Watch|    Fashion|  299.0|      Delhi|\n",
      "|       103|     Shoes|    Fashion| 2999.0|    Chennai|\n",
      "|       104|Smartphone|Electronics|14999.0|    Kolkata|\n",
      "|       106|       Oil|    Grocery|  110.0|    Chennai|\n",
      "|       107|    Laptop|Electronics|79999.0|  Bengalore|\n",
      "+----------+----------+-----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftsemiDF = productDF.join(customerDF, productDF.product_id == customerDF.product_id, \"leftsemi\")\n",
    "leftsemiDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43fa6b9-a874-449c-a292-92d9717df2b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Left Anti Join\n",
    "\n",
    "leftanti join does the exact opposite of the leftsemi, leftanti join returns only columns from the left dataset for non-matched records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0838720-9851-43f4-8ef0-530960307d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+------+-----------+\n",
      "|product_id| name|category| price|seller_city|\n",
      "+----------+-----+--------+------+-----------+\n",
      "|       102|  Bag| Fashion|1350.0|     Mumbai|\n",
      "|       105|Books|   Study| 145.0|      Delhi|\n",
      "+----------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftantiDF = productDF.join(customerDF, productDF.product_id == customerDF.product_id, \"leftanti\")\n",
    "leftantiDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84959f17-4fb8-468f-b8bc-afbfa6bf4030",
   "metadata": {},
   "source": [
    "# PySpark Window Functions\n",
    "\n",
    "PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row.\n",
    "\n",
    "| Window Functions Usage & Syntax | PySpark Window Functions description |\n",
    "| --- | --- |\n",
    "| row_number(): Column | Returns a sequential number starting from 1 within a window partition |\n",
    "| rank(): Column | Returns the rank of rows within a window partition, with gaps. |\n",
    "| percent_rank(): Column | Returns the percentile rank of rows within a window partition. |\n",
    "| dense_rank(): Column | Returns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps. |\n",
    "| ntile(n: Int): Column | Returns the ntile id in a window partition |\n",
    "| cume_dist(): Column | Returns the cumulative distribution of values within a window partition |\n",
    "| lag(columnName: String, offset: Int, defaultValue: Any): Column | returns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row. |\n",
    "| lead(columnName: String, offset: Int, defaultValue: Any): Column | returns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6eb08a99-caf1-4169-9933-ba4055f19a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b917a-9c96-4f01-9573-da4d0f818642",
   "metadata": {},
   "source": [
    "## row_number\n",
    "\n",
    "row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1af80391-7d24-4189-893c-77639baf13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+----+----------+\n",
      "|industry              |sector          |open|row_number|\n",
      "+----------------------+----------------+----+----------+\n",
      "|Agricultural Chemicals|Basic Industries|0.63|1         |\n",
      "|Agricultural Chemicals|Basic Industries|0.64|2         |\n",
      "|Agricultural Chemicals|Basic Industries|0.65|3         |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4         |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|5         |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|6         |\n",
      "|Agricultural Chemicals|Basic Industries|0.68|7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.69|8         |\n",
      "|Agricultural Chemicals|Basic Industries|0.69|9         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10        |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |11        |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |12        |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |13        |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |14        |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |15        |\n",
      "|Agricultural Chemicals|Basic Industries|0.71|16        |\n",
      "|Agricultural Chemicals|Basic Industries|0.71|17        |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|18        |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|19        |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|20        |\n",
      "+----------------------+----------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"industry\").orderBy(\"open\")\n",
    "\n",
    "data.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .select(\"industry\", \"sector\", \"open\", \"row_number\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f745fc-1b7d-40a4-b9de-5746ea5c4e10",
   "metadata": {},
   "source": [
    "## rank\n",
    "\n",
    "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "014523a4-38b8-4fcd-9944-0b45d2fe116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+----+----+\n",
      "|industry              |sector          |open|rank|\n",
      "+----------------------+----------------+----+----+\n",
      "|Agricultural Chemicals|Basic Industries|0.63|1   |\n",
      "|Agricultural Chemicals|Basic Industries|0.64|2   |\n",
      "|Agricultural Chemicals|Basic Industries|0.65|3   |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4   |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4   |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4   |\n",
      "|Agricultural Chemicals|Basic Industries|0.68|7   |\n",
      "|Agricultural Chemicals|Basic Industries|0.69|8   |\n",
      "|Agricultural Chemicals|Basic Industries|0.69|8   |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10  |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10  |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10  |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10  |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10  |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |10  |\n",
      "|Agricultural Chemicals|Basic Industries|0.71|16  |\n",
      "|Agricultural Chemicals|Basic Industries|0.71|16  |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|18  |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|18  |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|18  |\n",
      "+----------------------+----------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .select(\"industry\", \"sector\", \"open\", \"rank\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2be2e4-f443-4945-8776-8199121cd81b",
   "metadata": {},
   "source": [
    "## dense_rank\n",
    "\n",
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. *This is similar to rank() function difference being rank function leaves gaps in rank when there are ties*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45a432fd-cf20-4259-ab6b-ebb460c4cd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+----+----------+\n",
      "|industry              |sector          |open|dense_rank|\n",
      "+----------------------+----------------+----+----------+\n",
      "|Agricultural Chemicals|Basic Industries|0.63|1         |\n",
      "|Agricultural Chemicals|Basic Industries|0.64|2         |\n",
      "|Agricultural Chemicals|Basic Industries|0.65|3         |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4         |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4         |\n",
      "|Agricultural Chemicals|Basic Industries|0.67|4         |\n",
      "|Agricultural Chemicals|Basic Industries|0.68|5         |\n",
      "|Agricultural Chemicals|Basic Industries|0.69|6         |\n",
      "|Agricultural Chemicals|Basic Industries|0.69|6         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.7 |7         |\n",
      "|Agricultural Chemicals|Basic Industries|0.71|8         |\n",
      "|Agricultural Chemicals|Basic Industries|0.71|8         |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|9         |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|9         |\n",
      "|Agricultural Chemicals|Basic Industries|0.72|9         |\n",
      "+----------------------+----------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .select(\"industry\", \"sector\", \"open\", \"dense_rank\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b4fe5-3398-4555-b0d9-bf6233dcd26d",
   "metadata": {},
   "source": [
    "# PySpark SQL API\n",
    "\n",
    "The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame.\n",
    "\n",
    "## Registering DataFrames as Views\n",
    "\n",
    "To run SQL queries on Dataframe, **one has to register the dataframe as Temporary view**. This can be done as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed12bdcb-6d22-4788-a882-1093bae28360",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33549144-fa76-4781-a3de-2f6eacec3e51",
   "metadata": {},
   "source": [
    "To list views that are registred so far, use `spark.catalog.listTables()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97f58091-8362-4d6b-b677-be5ee026a1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stocks', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51133f36-fa2d-4fb4-a0b5-4fac1c6699c5",
   "metadata": {},
   "source": [
    "## Running SQL Queries\n",
    "\n",
    "Once the View is registered, you can run any SQL query using the `spark.sql(query)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cac12723-6fc8-4d36-8309-57589a7f9191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_star = spark.sql(\"SELECT * FROM stocks\")\n",
    "select_star.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00518d-d70c-4e37-a893-b8f7cf919f9a",
   "metadata": {},
   "source": [
    "## Where clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "220df6f0-24ce-4c90-bb1e-d11371123715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------+---------+\n",
      "|       sector|            industry|     open|    close|\n",
      "+-------------+--------------------+---------+---------+\n",
      "|Capital Goods|Biotechnology: La...|     54.0|    52.75|\n",
      "|Capital Goods|Biotechnology: La...|    52.75|    52.27|\n",
      "|Capital Goods|Biotechnology: La...|52.450001|55.200001|\n",
      "|Capital Goods|Biotechnology: La...|56.209999|56.779999|\n",
      "|Capital Goods|Biotechnology: La...|56.849998|     62.0|\n",
      "+-------------+--------------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"SELECT sector, industry, open, close FROM stocks where exchange = 'NASDAQ' and open > 50\"\"\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee7ebc-3d14-4d36-8742-710c3aa25210",
   "metadata": {},
   "source": [
    "## Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cc1f5b3-db8a-4f54-9124-5073b1c13d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----------+\n",
      "|              sector|         avg(open)| max(close)|\n",
      "+--------------------+------------------+-----------+\n",
      "|       Miscellaneous| 52.03839496900667|1035.829956|\n",
      "|         Health Care|119.96763306523248|   187000.0|\n",
      "|    Public Utilities|  35.5807773523944| 282.220001|\n",
      "|              Energy| 24.45658989126103| 901.039978|\n",
      "|Consumer Non-Dura...| 43.32860274612681| 664.130005|\n",
      "+--------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"SELECT sector, avg(open), max(close) FROM stocks group by 1\"\"\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f210634-91a3-4aec-9635-92eb1061fe05",
   "metadata": {},
   "source": [
    "# Writing Dataframe\n",
    "\n",
    "Once the data has been processed you can write the final dataframe into various data formats using the generic `DataFrame.write.format(format).save(filename)` method.\n",
    "\n",
    "## Saving in CSV format\n",
    "\n",
    "Saving data only for `Health Care` sector in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48d96251-2d79-4abe-a948-a17723909105",
   "metadata": {},
   "outputs": [],
   "source": [
    "health.write.format('csv').save('health')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6da75-66e9-4743-ac9c-281c854bfcc8",
   "metadata": {},
   "source": [
    "## Formats Supported\n",
    "\n",
    "- Parquet Files\n",
    "- ORC Files\n",
    "- JSON Files\n",
    "- CSV Files\n",
    "- Text Files\n",
    "- Hive Tables\n",
    "- JDBC To Other Databases\n",
    "- Avro Files and Many More"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8d09a-797d-4817-bed3-8b3a2c5f6289",
   "metadata": {},
   "source": [
    "## Partitioning Large Files\n",
    "\n",
    "PySpark partition is a way to split a large dataset into smaller datasets based on one or more partition keys. When you create a DataFrame from a file/table, based on certain parameters PySpark creates the DataFrame with a certain number of partitions in memory. This is one of the main advantages of PySpark DataFrame over Pandas DataFrame. Transformations on partitioned data run faster as they execute transformations parallelly for each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "852f1813-ad88-41d3-9785-b0813fa43d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "health.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"industry\") \\\n",
    "        .csv(\"industry_health_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30d8a3-433c-48f2-b532-51d601fba528",
   "metadata": {},
   "source": [
    "This will create seperate folders for each industry in which only data related to that industry will be stored. The ouput of the above cell will look like,\n",
    "\n",
    "![industry wise output](capture.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792426dc-7049-470f-9ef0-75239a82347b",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)\n",
    "- [Spark by Example](https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/)\n",
    "- [Spark Core Concepts](https://luminousmen.com/post/spark-core-concepts-explained)\n",
    "- [PySpark by Databricks](https://databricks.com/glossary/pyspark)\n",
    "- [PySpark Cheat Sheet by AWS](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
